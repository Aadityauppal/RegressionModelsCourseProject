---
title: 'Transmission choice for fuel economy: Manual OR Automatic'
author: "Aaditya Uppal"
date: "Tuesday, May 19, 2015"
output: html_document
---

##Executive Summary:

- *While looking for a fuel economic automobile, Manual transmission should be the vehicle of choice.*
- *The difference in mileage of cars is attributed to the weight and no. of cylinders in the car.*
- *Based on the observed data, mileage of Automatic transmission cars is about 7 miles per gallon (on average) less than Manual transmission cars.*
- *For every half a ton (~1000lbs) increase in the vehicle weight, there is a 3.21 mpg drop in the mileage.*
- *Mileage drops as we go for cars with more cylinders: For a 1-ton car, mileage goes down from 27.6 to 23.3 to 21.5 for 4, 6 and 8 cylinders respectively.*

```{r, echo=FALSE, warning=FALSE, results='hide'}
data(mtcars)
dat <- mtcars
str(dat)
head(dat)
names(dat)
unique(dat$drat)
unique(dat$vs)
unique(dat$gear)
unique(dat$carb)
sum(dat$am == "Automatic")
sum(dat$am == "Manual")
table(dat$vs)
```

```{r, echo=FALSE, warning=FALSE, results='hide'}
dat$cyl <- factor(dat$cyl)
dat$vs <- factor(dat$vs, labels = c("V", "S"))
dat$am <- factor(dat$am, labels = c("Automatic", "Manual"))
dat$gear <- factor(dat$gear)
dat$carb <- factor(dat$carb)
```

##Exploratory analysis:

We look at some plots first to understand how the data is distributed. The boxplot shows that at least for the second quartiles, mileage for Automatic transmission cars is always lower than the Manual transmission cars. Even the average mileage values confirm that. 

Assuming the data for both transmissions follow a T-distribution, we do a T-test for our Null hypothesis that mileage in the two cases is different and higher for manual transmission. Our T-test results also suggest that but we should also look at the effect of other parameters before making any conclusions.

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='none'}
library(ggplot2)
library(grid)
library(gridExtra)
boxplot(dat$mpg ~ dat$am, main = "Mileage for both transmissions", ylab = "Miles per Gallon (mpg)", xlab = "Transmission type", col = c("light blue", "sienna"))
mM <- round(mean(dat$mpg[grep("Manual", dat$am)]), 2)
mA <- round(mean(dat$mpg[grep("Automatic", dat$am)]), 2)
print(paste("Average mileage for Automatic transmission:", mA, "(mpg)", collapse = ""))
print(paste("Average mileage for Manual transmission:", mM, "(mpg)", collapse = ""))
#sd(dat$mpg[grep("Manual", dat$am)])
#sd(dat$mpg[grep("Automatic", dat$am)])
t.test(dat$mpg[grep("Manual", dat$am)] - dat$mpg[grep("Automatic", dat$am)])$estimate
t.test(dat$mpg[grep("Manual", dat$am)] - dat$mpg[grep("Automatic", dat$am)])$conf.int
p <- ggplot(dat, aes(wt, mpg)) + geom_point(aes(color = am), size = 2) 
p1 <- ggplot(dat, aes(cyl, mpg)) + geom_point(aes(color = am), size = 2)
p2 <- ggplot(dat, aes(disp, mpg)) + geom_point(aes(color = am), size = 2)
p3 <- ggplot(dat, aes(hp, mpg)) + geom_point(aes(color = am), size = 2) 
p4 <- ggplot(dat, aes(drat, mpg)) + geom_point(aes(color = am), size = 2) 
p5 <- ggplot(dat, aes(qsec, mpg)) + geom_point(aes(color = am), size = 2) 
p6 <- ggplot(dat, aes(vs, mpg)) + geom_point(aes(color = am), size = 2) 
p7 <- ggplot(dat, aes(gear, mpg)) + geom_point(aes(color = am), size = 2)
p8 <- ggplot(dat, aes(carb, mpg)) + geom_point(aes(color = am), size = 2)
grid.arrange(p, p1, p2, p3, p4, p5, p6, p7, p8, ncol = 2)
```   

##Regression modeling

We first analyse the correlation of *mileage* with other parameters. 

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='none'}
library(dplyr)
cordat <- c()
for(i in 2:ncol(mtcars)) {
  v1 <- mtcars$mpg
  v2 <- mtcars[, i]
  cordat[i-1] <- cor(v1, v2) 
}
names(cordat) <- names(select(dat, -1))
rank(cordat)
```

Let us build and analyse some models using predictors that have a high correlation with our outcome (mpg).

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='none'}
fit1 <- lm(mpg ~ wt, mtcars)
fit2 <- lm(mpg ~ wt + cyl, mtcars)
fit3 <- lm(mpg ~ wt + cyl + disp, mtcars)
fit4 <- lm(mpg ~ wt + cyl + disp + hp, mtcars)
fit5 <- lm(mpg ~ wt + cyl + disp + hp + carb, mtcars)
```

We analyse the standard errors, P-values (of predictors) and R-squared (%age of total variation explained by the model) values from the summary of these models (fits). We observe that the standard errors are increasing as more predictors are added. The P-values are also not significant beyond the **weight and cylinders** predictors. R-squared values on the other hand are constantly increasing which can be attributed to the addition of predictors. But if we observe the adjusted R-squared values, it dips beyond the adition of two predictors. Another observation is that as the **horsepower** predictor is added, standard error for the **weight** predictor actually reduces. The adjusted R-squared also increases again with the **horsepower** predictor. We add the same predictors once more but in a different order.

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='none'}
fit1a <- lm(mpg ~ wt, mtcars)
fit2a <- lm(mpg ~ wt + cyl, mtcars)
fit3a <- lm(mpg ~ wt + cyl + hp, mtcars)
fit4a <- lm(mpg ~ wt + cyl + hp + disp, mtcars)
fit5a <- lm(mpg ~ wt + cyl + hp + disp + carb, mtcars)
```

We again observe that the standard errors are increasing rapidly as more predictors are added. The P-values are significant only till the **weight and cylinders** predictors are included. R-squared values do not increase much beyond those two predictors. Adjusted R-squared values show a decline after three predictors. 

So now we select these three predictors - **weight, cylinders and horsepower**, for further analysis.

We conduct an *Analysis of Variance (ANOVA or nested likelyhood ratio)* test to study the suitability of these fits. The P-values indicate that the 3rd model does not add much significance to the prior models. These observations indicate that selecting **weight and cylinders** as predictors should give a suitable fit.

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='none'}
anova(fit1a, fit2a, fit3a)
fit <- lm(mpg ~ wt + cyl, dat)
coef(fit)
```

We look at the coefficients of our selected model now and try to plot the fit on top of the obeserved data.

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='none'}
plot(dat$mpg ~ dat$wt, col = dat$cyl)
abline(a = coef(fit)[1], b = coef(fit)[2])
abline(a = coef(fit)[1] + coef(fit)[3], b = coef(fit)[2], col = "red")
abline(a = coef(fit)[1] + coef(fit)[4], b = coef(fit)[2], col = "green")
```

We do **observe a good fit of our model on the recorded data**. But let us look at some residuals and diagnostic methods before making final conclusions.

##Residuals & Diagnostics

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='none'}
par(mfrow = c(2, 2))
plot(fit, which = 1)
plot(fit, which = 2)
plot(fit, which = 3)
plot(fit, which = 5)
```

Let us first look at some residuals plots. We do not observe any pattern in the residuals compared against the fitted values. The residuals also seem to have a normal distribution in the theoretical quantiles. We also do not spot any high residuals as a result of high influence (leverage) points. Finally, let us look at some influence measures for our selected model.

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='none'}
(max(hatvalues(fit)) - mean(hatvalues(fit)))/mean(hatvalues(fit))
max(abs(dffits(fit)))/mean(predict(fit))
mean(abs(dfbetas(fit)[, 2]))/abs(coef(fit)[2])
max(cooks.distance(fit))
```

**The most influential point has a hatvalue less than twice the mean hatvalues. The maximum change in predicted response is less than 5% of the mean predicted values. The mean change in the weight coefficient values is only about 3% of the predicted coefficient. The overall change in the coefficients is only about 0.23** *Our residuals and diagnostic tests do suggest that our model is a good fit*.

##Conclusions & Inference

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='none'}
summary(fit)
```

Let us look at our model once again. It contains two predictors: one continuous (weight) and the other discrete (no. of cylinders), a factor variable. *For a 4-cylinder car with a given weight, our model predicts a linear fit with slope -3.206 and an intercept of 33.99; For a 6 or 8-cylinder car, our model predicts a linear fit with same slope but intercepts of 29.74 and 27.92 respectively.* So our model predicts:

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='asis', fig.keep='none'}
mpg4cyl <- round(coef(fit)[1] + 2 * coef(fit)[2], 2)
mpg6cyl <- round(coef(fit)[1] + coef(fit)[3] + 2 * coef(fit)[2], 2)
mpg8cyl <- round(coef(fit)[1] + coef(fit)[4] + 2 * coef(fit)[2], 2)
print(paste("Mileage for a 1-ton (~2000lbs) 4-cylinder car:", mpg4cyl, "mpg", collapse = ""))
print(paste("Mileage for a 1-ton 6-cylinder car:", mpg6cyl, "mpg", collapse = ""))
print(paste("Mileage for a 1-ton 8-cylinder car:", mpg8cyl, "mpg", collapse = ""))
```

In conclusion, **Mileage of cars depend on weight and no. of cylinders. Even though the mileage of Automatic cars is observed to be lower than that of Manual transmission, it can be attributed to those two predictors. Our model does carry some level of uncertainty associated with limited data. More test data can be collected to further develop the model.**

#Appendix

##Exploratory analysis:

```{r, echo=TRUE, warning=FALSE, message=FALSE, results='asis'}
boxplot(dat$mpg ~ dat$am, main = "Mileage for both transmissions", ylab = "Miles per Gallon (mpg)", xlab = "Transmission type", col = c("light blue", "sienna"))
mM <- round(mean(dat$mpg[grep("Manual", dat$am)]), 2)
mA <- round(mean(dat$mpg[grep("Automatic", dat$am)]), 2)
print(paste("Average mileage for Automatic transmission:", mA, "(mpg)", collapse = ""))
print(paste("Average mileage for Manual transmission:", mM, "(mpg)", collapse = ""))
t.test(dat$mpg[grep("Manual", dat$am)] - dat$mpg[grep("Automatic", dat$am)])$estimate
t.test(dat$mpg[grep("Manual", dat$am)] - dat$mpg[grep("Automatic", dat$am)])$conf.int
```

##Regression modeling

```{r, echo=TRUE, warning=FALSE, message=FALSE, results='markup', fig.keep='none'}
library(dplyr)
cordat <- c()
for(i in 2:ncol(mtcars)) {
  v1 <- mtcars$mpg
  v2 <- mtcars[, i]
  cordat[i-1] <- cor(v1, v2) 
}
names(cordat) <- names(select(dat, -1))
rank(cordat)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='none'}
fit1 <- lm(mpg ~ wt, mtcars)
fit2 <- lm(mpg ~ wt + cyl, mtcars)
fit3 <- lm(mpg ~ wt + cyl + disp, mtcars)
fit4 <- lm(mpg ~ wt + cyl + disp + hp, mtcars)
fit5 <- lm(mpg ~ wt + cyl + disp + hp + carb, mtcars)
```

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide', fig.keep='none'}
fit1a <- lm(mpg ~ wt, mtcars)
fit2a <- lm(mpg ~ wt + cyl, mtcars)
fit3a <- lm(mpg ~ wt + cyl + hp, mtcars)
fit4a <- lm(mpg ~ wt + cyl + hp + disp, mtcars)
fit5a <- lm(mpg ~ wt + cyl + hp + disp + carb, mtcars)
```

```{r, echo=TRUE, warning=FALSE, message=FALSE, results='markup', fig.keep='none'}
anova(fit1a, fit2a, fit3a)
fit <- lm(mpg ~ wt + cyl, dat)
coef(fit)
```

```{r, echo=TRUE, warning=FALSE, message=FALSE, results='hide'}
plot(dat$mpg ~ dat$wt, col = dat$cyl)
abline(a = coef(fit)[1], b = coef(fit)[2])
abline(a = coef(fit)[1] + coef(fit)[3], b = coef(fit)[2], col = "red")
abline(a = coef(fit)[1] + coef(fit)[4], b = coef(fit)[2], col = "green")
```

##Residuals & Diagnostics

```{r, echo=FALSE, warning=FALSE, message=FALSE, results='hide'}
par(mfrow = c(2, 2))
plot(fit, which = 1)
plot(fit, which = 2)
plot(fit, which = 3)
plot(fit, which = 5)
```

```{r, echo=TRUE, warning=FALSE, message=FALSE, results='hide'}
(max(hatvalues(fit)) - mean(hatvalues(fit)))/mean(hatvalues(fit))
max(abs(dffits(fit)))/mean(predict(fit))
mean(abs(dfbetas(fit)[, 2]))/abs(coef(fit)[2])
max(cooks.distance(fit))
```